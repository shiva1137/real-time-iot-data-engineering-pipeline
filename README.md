# Real-Time IoT Data Engineering Pipeline: Streaming Data Processing & Analytics

A production-grade, interview-ready real-time data pipeline for IoT sensor data processing. This project demonstrates end-to-end data engineering practices from ingestion to serving analytics via REST API.

## ğŸ“Š Project Overview

This project processes **100 IoT sensors** generating data every **10 seconds**, resulting in approximately **864,000 readings per day**. The pipeline handles real-time streaming, batch processing, data quality validation, transformations, and API services.

### Key Metrics
- **Devices**: 100 IoT sensors
- **Data Frequency**: Every 10 seconds
- **Daily Volume**: ~864,000 readings/day
- **Data Types**: Temperature, Humidity, Energy Consumption

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   IoT Sensor Data (Faker Generator)                         â”‚
â”‚   - Temperature, Humidity, Energy Consumption               â”‚
â”‚   - 100 devices, every 10 seconds, continuous               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Kafka Topic: raw_iot     â”‚
        â”‚   (Multi-partition)        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                           â”‚
        â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SPARK STREAMING  â”‚      â”‚ SPARK BATCH      â”‚
â”‚ (5-min windows)  â”‚      â”‚ (Daily job)      â”‚
â”‚ - Aggregations   â”‚      â”‚ - Features       â”‚
â”‚ - Late data      â”‚      â”‚ - Cleaning       â”‚
â”‚ - Deduplication  â”‚      â”‚ - Aggregations   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Storage Layer          â”‚
        â”‚  - MongoDB (Write)      â”‚
        â”‚  - PostgreSQL (Read)    â”‚
        â”‚  - raw_iot              â”‚
        â”‚  - cleaned_iot          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  dbt Transformations    â”‚
        â”‚  - Staging              â”‚
        â”‚  - Intermediate         â”‚
        â”‚  - Marts (Analytics)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Analytics DB           â”‚
        â”‚  (PostgreSQL marts)     â”‚
        â”‚  (Read operations)      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚            â”‚            â”‚
        â–¼            â–¼            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ FastAPIâ”‚ â”‚ Airflow â”‚ â”‚Monitoringâ”‚
    â”‚ REST   â”‚ â”‚Orchestr.â”‚ â”‚  Logs    â”‚
    â”‚ API    â”‚ â”‚         â”‚ â”‚  Alerts  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose
- Python 3.9+
- Git

### Setup Instructions

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd "IOT Data Engineering Project"
   ```

2. **Configure environment variables**
   ```bash
   cp .env.example .env
   # Edit .env with your configuration
   ```

3. **Start infrastructure services**
   ```bash
   cd docker
   docker-compose up -d
   ```

4. **Verify services are running**
   ```bash
   docker-compose ps
   ```

   You should see:
   - MongoDB: `localhost:27017`
   - PostgreSQL: `localhost:5432`
   - Kafka: `localhost:9092`
   - Kafka UI: `localhost:8080` (optional)

5. **Initialize Kafka topics**
   ```bash
   # Install dependencies
   cd kafka
   pip install -r requirements.txt
   
   # Initialize topics
   python init_topics.py
   
   # Or verify topics exist:
   docker exec -it iot_kafka kafka-topics.sh --list --bootstrap-server localhost:9092
   ```

6. **Run data generator and producer** (Topic 2)
   ```bash
   cd ../data_generator
   pip install -r requirements.txt
   python producer.py
   ```

7. **Run validation consumer** (in separate terminal)
   ```bash
   cd ../data_quality
   pip install -r requirements.txt
   python validation_consumer.py
   ```

See [docs/topic2_usage_guide.md](docs/topic2_usage_guide.md) for detailed usage instructions.

## ğŸ“ Project Structure

```
â”œâ”€â”€ data_generator/          # Topic 2: Faker data generator + Kafka producer
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ generator.py         # India-based data generator with quality issues
â”‚   â”œâ”€â”€ producer.py          # Idempotent Kafka producer
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ kafka/                   # Topic 2: Kafka configs
â”‚   â”œâ”€â”€ topics_config.json
â”‚   â”œâ”€â”€ init_topics.py       # Topic initialization script (Python)
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ spark_streaming/         # Topic 3: Real-time processing
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ streaming_job.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ spark_batch/             # Topic 4: Daily batch jobs
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ batch_job.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ data_quality/            # Topic 2 & 5: Validation logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ validators.py       # Validation functions (Topic 5)
â”‚   â”œâ”€â”€ validation_consumer.py  # Kafka validation consumer (Topic 2)
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ dbt/                     # Topic 6: dbt project
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ intermediate/
â”‚   â”‚   â””â”€â”€ marts/
â”‚   â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â””â”€â”€ profiles.yml
â”‚
â”œâ”€â”€ api/                     # Topic 7: FastAPI
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ routes/
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ airflow/                 # Topic 8: DAGs
â”‚   â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ plugins/
â”‚
â”œâ”€â”€ monitoring/              # Topic 9: Logs, alerts
â”‚   â”œâ”€â”€ logging_config.py
â”‚   â””â”€â”€ alerts.py
â”‚
â”œâ”€â”€ docker/                  # Topic 10: Dockerfiles
â”‚   â”œâ”€â”€ Dockerfile.generator
â”‚   â”œâ”€â”€ Dockerfile.spark
â”‚   â”œâ”€â”€ Dockerfile.api
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ .github/                 # Topic 11: CI/CD
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml
â”‚
â”œâ”€â”€ docs/                    # Documentation
â”‚   â”œâ”€â”€ architecture.md      # System architecture
â”‚   â”œâ”€â”€ topic1_comprehensive_guide.md  # Topic 1: Complete guide
â”‚   â”œâ”€â”€ topic2_comprehensive_guide.md  # Topic 2: Complete guide
â”‚   â””â”€â”€ topic2_usage_guide.md          # Topic 2: Usage guide
â”‚
â”œâ”€â”€ scripts/                 # Utility scripts
â”‚   â””â”€â”€ setup.py             # Project setup script (Python, uses stdlib only)
â”‚
â”œâ”€â”€ tests/                   # Test files
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ”§ Technology Stack

- **Message Queue**: Apache Kafka (KRaft mode)
- **Streaming**: Apache Spark Streaming
- **Batch Processing**: Apache Spark (PySpark)
- **Write Database**: MongoDB
- **Read Database**: PostgreSQL
- **Transformations**: dbt (Data Build Tool)
- **API**: FastAPI
- **Orchestration**: Apache Airflow
- **Containerization**: Docker & Docker Compose

## ğŸ“š Topics Covered

### âœ… Completed Topics

1. **âœ… Project Setup & Architecture** - Complete
   - Complete folder structure following best practices
   - Docker Compose setup with MongoDB, PostgreSQL, Kafka (KRaft)
   - Comprehensive documentation and architecture diagrams
   - Git repository initialization
   - CI/CD pipeline setup

2. **âœ… Data Ingestion with Kafka** - Complete
   - Kafka broker setup in KRaft mode (no Zookeeper)
   - India-based Faker data generator (100 sensors across major cities)
   - Comprehensive data quality issues (nulls, duplicates, late data, out-of-range, type mismatches, schema violations, formatting)
   - Idempotent Kafka producer (MVP - clean, focused implementation)
     - Schema validation (lightweight checks)
     - Retry logic with exponential backoff (5 retries max)
     - Partition hashing for ordering per sensor
     - Thread-safe statistics tracking
   - Validation consumer with comprehensive quality checks
   - Dead-letter queue (DLQ) for invalid data
   - Quality metrics tracking
   - Interview preparation documentation

### ğŸš§ In Progress / Upcoming Topics

3. **Real-Time Processing** - Upcoming
   - Spark Streaming, windowing, aggregations
   - Late data handling with watermarking
   - State management

4. **Batch Processing** - Upcoming
   - Daily jobs, feature engineering
   - Data cleaning and transformations

5. **Data Quality** - Upcoming
   - Validation, schema enforcement
   - Dead-letter queue (DLQ) pattern

6. **dbt Transformations** - Upcoming
   - Staging, intermediate, marts
   - SQL-based transformations

7. **FastAPI** - Upcoming
   - REST API for data access
   - Pydantic validation

8. **Airflow Orchestration** - Upcoming
   - DAGs, scheduling
   - Workflow management

9. **Monitoring & Logging** - Upcoming
   - Alerts, observability
   - Structured logging

10. **Docker** - Upcoming
    - Containerization
    - Multi-stage builds

11. **CI/CD** - Upcoming
    - GitHub Actions
    - Automated testing

12. **Production Deployment** - Upcoming
    - Best practices
    - Performance optimization

## ğŸ“Š Project Progress

**Overall Progress: 2/12 Topics (17%)**

- âœ… Topic 1: Project Setup & Architecture
- âœ… Topic 2: Data Ingestion with Kafka
- â³ Topic 3-12: In Development

## ğŸ¯ Interview Preparation

This project is designed to answer common data engineering interview questions:

- **Architecture**: "Walk me through your data pipeline architecture"
- **Technology Choices**: "Why Kafka over RabbitMQ/Redis?"
- **Data Flow**: "How does data flow through your system?"
- **Scalability**: "How would you scale this pipeline?"
- **Failure Handling**: "What happens if Kafka/MongoDB/PostgreSQL fails?"

**Comprehensive Documentation:**
- [Topic 1 Comprehensive Guide](docs/topic1_comprehensive_guide.md) - Project setup & architecture
- [Topic 2 Comprehensive Guide](docs/topic2_comprehensive_guide.md) - Data ingestion with Kafka
- [Topic 2 Usage Guide](docs/topic2_usage_guide.md) - Practical setup & usage
- [Architecture Documentation](docs/architecture.md) - System design & decisions

## ğŸ¤ Contributing

This is a learning project. Feel free to fork and experiment!

## ğŸ“ License

This project is for educational purposes.

## ğŸ”— Useful Links

- Kafka UI: http://localhost:8080
- FastAPI Docs: http://localhost:8000/docs (when running)
- Airflow UI: http://localhost:8080/airflow (when running)

---

## ğŸ¯ Project Status

**Current Status**: âœ… Topics 1-2 Complete | ğŸš§ Topics 3-12 In Development

**Last Updated**: January 2025

**Repository**: [GitHub - Real-Time IoT Data Engineering Pipeline](https://github.com/shiva1137/real-time-iot-data-engineering-pipeline)

---

## ğŸ“ˆ Learning Journey

This project is part of a structured learning path to master data engineering concepts through hands-on implementation. Each topic builds upon the previous one, creating a complete, production-ready pipeline.

**Key Achievements So Far:**
- âœ… Production-grade project structure
- âœ… Docker infrastructure setup
- âœ… Comprehensive architecture documentation
- âœ… Kafka broker in KRaft mode
- âœ… India-based data generator with comprehensive quality issues
- âœ… Idempotent Kafka producer with error handling
- âœ… Validation consumer with DLQ pattern
- âœ… Interview-ready explanations and Q&A

**Next Milestones:**
- ğŸ¯ Topic 3: Build Spark Streaming pipeline (consumes from validated_iot_data)
- ğŸ¯ Topic 4: Create batch processing jobs
- ğŸ¯ Topic 5: Enhanced data quality validation

